{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - DQN-SpaceInvaders.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hesller/Bean-factory-and-Application-Context/blob/master/1_DQN_SpaceInvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "V8ERgDiMWoaN"
      },
      "cell_type": "code",
      "source": [
        "# Install the prerequired libraries\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0OCC4_OsV696"
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Apr 20 14:23:46 2019\n",
        "\n",
        "@author: Hesller Huller\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "    Step 1 - Import libraries\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import transform \n",
        "from skimage.color import rgb2gray \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv2D, Flatten\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import gym\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "corBDFanjeKY"
      },
      "cell_type": "code",
      "source": [
        "print(\"tensorflow version\")\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lmzuyeQAZ8jy"
      },
      "cell_type": "code",
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjwRkNjDaJO0",
        "outputId": "f5a6ffa4-87a4-4db6-b150-ad9c9f91efb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Is there a GPU available: \"),\n",
        "print(tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is there a GPU available: \n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7ysaJ0zpWmwj",
        "outputId": "03e3b6bd-085e-4d46-a624-64a3cbf3694e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Step 2 - Create the environment\n",
        "\"\"\"\n",
        "env = gym.make(\"SpaceInvaders-v0\")\n",
        "print(\"The size of the environment: \", env.observation_space)\n",
        "print(\"Number of actions: \", env.action_space.n)\n",
        "a = env.action_space.n\n",
        "# Hot encoding actions\n",
        "possible_actions = np.array(np.identity(env.action_space.n, dtype=int).tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of the environment:  Box(210, 160, 3)\n",
            "Number of actions:  6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8M1xP7cAWj_T"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Step 3 - Preprocess frames\n",
        "\"\"\"\n",
        "def preprocess_frame(frame):\n",
        "    '''\n",
        "        This functions pre process the frame.\n",
        "        1 - Pick the frame\n",
        "        2 - Grayscale it\n",
        "        3 - resize it\n",
        "        4 - normalize it\n",
        "\n",
        "        Args:\n",
        "            frame - an input frame\n",
        "        \n",
        "        Returns:\n",
        "            preprocessed_frame - gray, scaled, resized frame\n",
        "    '''\n",
        "    # Grayscale frame\n",
        "    gray = rgb2gray(frame)\n",
        "\n",
        "    # Resize - cropping the unused part of the frame\n",
        "    # [UP: DOWN: LEFT: RIGHT]\n",
        "    cropped_frame = gray[8:-12, 4:-12]\n",
        "\n",
        "    # Normalize pixel values\n",
        "    normalized_frame = cropped_frame / 255.0\n",
        "\n",
        "    # Resize\n",
        "    preprocessed_frame = transform.resize(normalized_frame, [110,84])\n",
        "\n",
        "    return preprocessed_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8RSiXL6Wf93"
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "#   Stacking frames\n",
        "#\n",
        "stack_size = 4\n",
        "# Initialize the deque with zeros image, one array for each image\n",
        "stacked_frames = deque([np.zeros((110,84), dtype = np.int) for i in range(stack_size)], maxlen=stack_size)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        \n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "        \n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ncVMwBJqWbqp"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Step 4: Set up our hyperparameters\n",
        "\"\"\"\n",
        "\n",
        "### MODEL HYPERPARAMETERS\n",
        "state_size = [110, 84, 4]      # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels) \n",
        "action_size = env.action_space.n # 8 possible actions\n",
        "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
        "\n",
        "### TRAINING HYPERPARAMETERS\n",
        "total_episodes = 50            # Total episodes for training\n",
        "max_steps = 50000              # Max possible steps in an episode\n",
        "batch_size = 64                # Batch size\n",
        "\n",
        "# Exploration parameters for epsilon greedy strategy\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability \n",
        "decay_rate = 0.00001           # exponential decay rate for exploration prob\n",
        "\n",
        "# Q learning hyperparameters\n",
        "gamma = 0.9                    # Discounting rate\n",
        "\n",
        "### MEMORY HYPERPARAMETERS\n",
        "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
        "memory_size = 1000000          # Number of experiences the Memory can keep\n",
        "\n",
        "### PREPROCESSING HYPERPARAMETERS\n",
        "stack_size = 4                 # Number of frames stacked\n",
        "\n",
        "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
        "training = False\n",
        "\n",
        "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
        "episode_render = False\n",
        "\n",
        "# Size of the network\n",
        "NET = 'bigger' # smaller"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uP075XokWWCV"
      },
      "cell_type": "code",
      "source": [
        "# DEfine the network object\n",
        "class DQNetwork():\n",
        "    \n",
        "    def __init__(self, scope='QNet', VALID_ACTIONS=[0,1,2,3,4,5], NET=NET):\n",
        "        self.NET = NET\n",
        "        self.scope = scope\n",
        "        self.VALID_ACTIONS = VALID_ACTIONS\n",
        "        self._build_model()\n",
        "        self.epsilon = explore_start\n",
        "        self.epsilon_min = explore_stop\n",
        "        self.decay_rate = decay_rate\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = 20\n",
        "        # define memory size\n",
        "        self.buffer = deque(maxlen = memory_size)\n",
        "        # initialize the checkpoint\n",
        "        self.checkpoint = ModelCheckpoint(\n",
        "                          'models/{}.model'.format(\n",
        "                          'DQN-SpaceInvaders-{epoch:02d}-Loss-{loss:.6f}', \n",
        "                          monitor=['loss'],\n",
        "                          interval=10,\n",
        "                          verbose=1, \n",
        "                          save_best_only=True, \n",
        "                          mode='max'))\n",
        "    def _build_model(self):\n",
        "        self.model = Sequential()\n",
        "        if (self.NET == 'bigger'):\n",
        "            # this is the 3 convnetwork\n",
        "            self.model.add(Conv2D(input_shape=(*state_size,), filters=32, kernel_size=8, strides=(4,4), padding='VALID', activation='relu', name='conv1'))\n",
        "            self.model.add(Conv2D(filters=64, kernel_size=4, strides=(2,2), padding='VALID', activation='relu', name='conv2'))\n",
        "            self.model.add(Conv2D(filters=64, kernel_size=3, strides=(1,1), padding='VALID', activation='relu'))\n",
        "            # fully connected layers\n",
        "            self.model.add(Flatten())\n",
        "            self.model.add(Dense(512, activation='relu'))\n",
        "            self.model.add(Dense(len(self.VALID_ACTIONS)))\n",
        "            self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
        "        \n",
        "        elif(self.NET == 'smaller'):\n",
        "            self.model.add(Conv2D(filters=16, kernel_size=8, strides=(4,4), padding='VALID', activation='relu', name='conv2'))\n",
        "            self.model.add(Conv2D(filters=32, kernel_size=32, strides=(2,2), padding='VALID', activation='relu'))\n",
        "            # fully connected layers\n",
        "            self.model.add(Flatten())\n",
        "            self.model.add(Dense(256, activation='relu'))\n",
        "            self.model.add(Dense(len(self.VALID_ACTIONS)))\n",
        "            self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randint(1, len(possible_actions))-1\n",
        "        state = np.reshape(state, [1, *state.shape])\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # reshaping\n",
        "        state_m = np.reshape(state, (1, *state.shape))\n",
        "        next_state_m = np.reshape(next_state, (1, *next_state.shape))\n",
        "        self.buffer.append((state_m, action, reward, next_state_m, done))\n",
        "        \n",
        "    def replay(self, batch_size, decay_step, episode):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return \n",
        "        \n",
        "        minibatch = random.sample(self.buffer, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done: # if the game is not over\n",
        "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "            # when teh game is over\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            \n",
        "            # saving a checkpoint every 10 episodes\n",
        "            if episode % 10 == 0:\n",
        "              self.model.fit(state, target_f, verbose=0, \n",
        "                             batch_size=self.batch_size, max_queue_size=5,\n",
        "                             callbacks=[self.checkpoint])\n",
        "            else:\n",
        "              self.model.fit(state, target_f, verbose=0, \n",
        "                             batch_size=self.batch_size, max_queue_size=5)\n",
        "              \n",
        "        # updating the exploration rate   \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon = self.epsilon_min + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
        "            print('the self.epsilon: ', self.epsilon )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMIKoNVqWPda"
      },
      "cell_type": "code",
      "source": [
        "# Create the network object\n",
        "network = DQNetwork()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPMX-CDinWor"
      },
      "cell_type": "code",
      "source": [
        "# create models directory\n",
        "!mkdir models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Cjln9BnWKob"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "    Pre-populating the memory\n",
        "    \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "for i in range(pretrain_length):\n",
        "    # if it is the first step\n",
        "    if i == 0:\n",
        "        state = env.reset()\n",
        "        \n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "        \n",
        "    # Get the next state, rewards, and done by taking one step\n",
        "    choice = random.randint(1, len(possible_actions))-1\n",
        "    next_state, reward, done, _ = env.step(choice)\n",
        "    \n",
        "    env.render()\n",
        "    \n",
        "    # Stack the next_state frame\n",
        "    state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "    \n",
        "    # if episode is finished we are dead 3x\n",
        "    if done:\n",
        "        # We finish the episode\n",
        "        next_state = np.zeros(state.shape)\n",
        "        \n",
        "        # Stack the frames\n",
        "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "        \n",
        "        # reshaping\n",
        "        state_m = np.reshape(state, (1, *state.shape))\n",
        "        next_state_m = np.reshape(next_state, (1, *next_state.shape))\n",
        "        \n",
        "        # add the experience to memory\n",
        "        network.remember(state_m, choice, reward, next_state_m, done)\n",
        "        \n",
        "        # Start a new episode\n",
        "        state = env.reset()\n",
        "        \n",
        "        # Stack the frames\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "    \n",
        "    else:\n",
        "        # reshaping to save\n",
        "        # reshaping\n",
        "        state_m = np.reshape(state, (1, *state.shape))\n",
        "        next_state_m = np.reshape(next_state, (1, *next_state.shape))\n",
        "        \n",
        "        # add the experience to the memory\n",
        "        network.remember((state_m, choice, reward, next_state_m, done))\n",
        "        \n",
        "        # the state is now the new_state\n",
        "        state = next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSZfe3cWWGUw"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Train the environment\n",
        "    \n",
        "\"\"\"\n",
        "\n",
        "decay_step = 0\n",
        "for e in range(5000):\n",
        "    \n",
        "    # Initialize the rewards of the episode\n",
        "    episode_rewards = []\n",
        "    \n",
        "    # reset the environment\n",
        "    state = env.reset()\n",
        "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "    \n",
        "    \n",
        "    for s in range(max_steps):\n",
        "        \n",
        "        decay_step += 1\n",
        "        \n",
        "        # rendering the environment\n",
        "        #env.render()\n",
        "        \n",
        "        # choose an action\n",
        "        action = network.act(state)\n",
        "        \n",
        "        # Perform the action the reward and next state and done\n",
        "        next_state, reward, done, _  = env.step(action)\n",
        "        \n",
        "        # Add the reward to total pool\n",
        "        episode_rewards.append(reward)\n",
        "        \n",
        "        # if done finish the game\n",
        "        if done:\n",
        "            # the end of the episode, so there is no next_state\n",
        "            next_state = np.zeros((110, 84), dtype=np.int)\n",
        "            \n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "            \n",
        "            \n",
        "            # equals the step to ende the episode\n",
        "            print('game finished')\n",
        "            \n",
        "            # Get the total reward of the episode\n",
        "            total_reward = np.sum(episode_rewards)\n",
        "            print('Episode: {}'.format(e),\n",
        "                                  'Total reward: {}'.format(total_reward),\n",
        "                                  'Explore P: {:.4f}'.format(network.epsilon))\n",
        "            \n",
        "            # store the transition\n",
        "            network.remember(state, action, reward, next_state, done)\n",
        "\n",
        "            break\n",
        "        else:\n",
        "            # inside playing the game store the states\n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "            \n",
        "            # store the experience\n",
        "            network.remember(state, action, reward, next_state, done)\n",
        "            \n",
        "            state = next_state\n",
        "            \n",
        "    # train the agent with the experience of the episode\n",
        "    network.replay(32, decay_step, e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-OKRDIf0c5gz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ac7392b-c4d7-4540-d517-c04dfa9aabaf"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IoBaUxnxdAKi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "779b0208-3f80-4bcd-8c12-ad995ae022a9"
      },
      "cell_type": "code",
      "source": [
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, log={}):\n",
        "    if(log.get('acc')>0.6):\n",
        "      print(\"\\nReached the end of the trainin with 60% or above accuracy\")\n",
        "      self.model.stop_training = True\n",
        "      \n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0 , x_test / 255.0\n",
        "\n",
        "callbacks = MyCallback()\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, verbose = 1, callbacks=[callbacks])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "59936/60000 [============================>.] - ETA: 0s - loss: 0.5907 - acc: 0.8073\n",
            "Reached the end of the trainin with 60% or above accuracy\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5906 - acc: 0.8073\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff810730048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "yi1rPbx4djA1"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}